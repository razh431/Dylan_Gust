{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "# Set the list of wind project names and the words to search for\n",
    "with open('wind_projects.csv', newline='') as csvfile:\n",
    "    wp_reader = csv.reader(csvfile)\n",
    "    wind_project_names = [row[0] for row in wp_reader]\n",
    "\n",
    "# Set the search words from the CSV\n",
    "with open('search_words.csv', newline='') as csvfile:\n",
    "    sw_reader = csv.reader(csvfile)\n",
    "    search_words = [row[0] for row in sw_reader]\n",
    "\n",
    "# Set the website URLs from the CSV\n",
    "with open('website_urls.csv', newline='') as csvfile:\n",
    "    url_reader = csv.reader(csvfile)\n",
    "    website_urls = [row[0] for row in url_reader]\n",
    "\n",
    "# Create an empty list to store the press release links\n",
    "pr_links = []\n",
    "\n",
    "# Create a set to store the visited URLs, so we don't visit the same URL twice\n",
    "visited_urls = set()\n",
    "\n",
    "# Loop through each URL and send a GET request to the website and store the HTML response\n",
    "for url in website_urls:\n",
    "    # Add the URL to the visited set\n",
    "    visited_urls.add(url)\n",
    "\n",
    "    # Send a GET request to the website and store the HTML response\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Parse the HTML content using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Find all links on the page\n",
    "    links = soup.find_all('a')\n",
    "\n",
    "    # Loop through each link and check if it is a press release for a wind project that mentions any of the search words\n",
    "    for link in links:\n",
    "        href = link.get('href')\n",
    "        if href and 'press-release' in href:\n",
    "            # If the link doesn't start with 'http' or 'https', assume it's a relative link and construct the full URL\n",
    "            if not href.startswith('http') and not href.startswith('https'):\n",
    "                url_parts = urlparse(url)\n",
    "                href = f'{url_parts.scheme}://{url_parts.netloc}{href}'\n",
    "\n",
    "            # Check if the link is on the same site as the initial URL\n",
    "            if urlparse(href).netloc == urlparse(url).netloc:\n",
    "                for wind_project_name in wind_project_names:\n",
    "                    if wind_project_name.lower() in href.lower():\n",
    "                        # Send a GET request to the press release link and store the HTML response\n",
    "                        pr_response = requests.get(href)\n",
    "\n",
    "                        # Parse the HTML content using BeautifulSoup\n",
    "                        pr_soup = BeautifulSoup(pr_response.content, 'html.parser')\n",
    "\n",
    "                        # Extract the relevant information from the press release\n",
    "                        title = pr_soup.find('h1').text\n",
    "                        date = pr_soup.find('span', {'class': 'date'}).text\n",
    "                        content = pr_soup.find('div', {'class': 'content'}).text\n",
    "\n",
    "                        # Check if the press release mentions any of the search words\n",
    "                        for search_word in search_words:\n",
    "                            if search_word in content.lower():\n",
    "                                # Add the press release link to the list\n",
    "                                pr_links.append(href)\n",
    "\n",
    "    # Find all links on the page\n",
    "    links = soup.find_all('a')\n",
    "\n",
    "    # Loop through each link and check if it is a relative URL and is rooted at the initial URL\n",
    "    for link in links:\n",
    "        href = link.get('href')\n",
    "        if href and not href.startswith('http') and not href.startswith('https'):\n",
    "            full_url = f'{urlparse(url).scheme}://{urlparse(url).netloc}{href}'\n",
    "            if full_url not in visited_urls:\n",
    "                visited_urls.add(full_url)\n",
    "                # Check if the link is on the same site as the initial URL\n",
    "                if urlparse(full_url).netloc == urlparse(url).netloc:\n",
    "                    # Add the full URL to the list of website URLs to visit\n",
    "                    website_urls.append(full_url)\n",
    "\n",
    "# Loop through each URL in the website URLs list\n",
    "for url in website_urls:\n",
    "    # Add the URL to the visited set\n",
    "    visited_urls.add(url)\n",
    "\n",
    "    # Send a GET request to the website and store the HTML response\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Parse the HTML content using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Find all links on the page\n",
    "    links = soup.find_all('a')\n",
    "\n",
    "    # Loop through each link and check if it is a press release for a wind project that mentions any of the search words\n",
    "    for link in links:\n",
    "        href = link.get('href')\n",
    "        if href and 'press-release' in href:\n",
    "            # If the link doesn't start with 'http' or 'https', assume it's a relative link and construct the full URL\n",
    "            if not href.startswith('http') and not href.startswith('https'):\n",
    "                url_parts = urlparse(url)\n",
    "                href = f'{url_parts.scheme}://{url_parts.netloc}{href}'\n",
    "\n",
    "            # Check if the link is on the same site as the initial URL\n",
    "            if urlparse(href).netloc == urlparse(url).netloc:\n",
    "                for wind_project_name in wind_project_names:\n",
    "                    if wind_project_name.lower() in href.lower():\n",
    "                        # Send a GET request to the press release link and store the HTML response\n",
    "                        pr_response = requests.get(href)\n",
    "\n",
    "                        # Parse the HTML content using BeautifulSoup\n",
    "                        pr_soup = BeautifulSoup(pr_response.content, 'html.parser')\n",
    "\n",
    "                        # Extract the relevant information from the press release\n",
    "                        title = pr_soup.find('h1').text\n",
    "                        date = pr_soup.find('span', {'class': 'date'}).text\n",
    "                        content = pr_soup.find('div', {'class': 'content'}).text\n",
    "\n",
    "                        # Check if the press release mentions any of the search words\n",
    "                        for search_word in search_words:\n",
    "                            if search_word in content.lower():\n",
    "                                # Add the press release link to the list\n",
    "                                pr_links.append(href)\n",
    "\n",
    "    # Find all links on the page\n",
    "    links = soup.find_all('a')\n",
    "\n",
    "    # Loop through each link and check if it is a relative URL and is rooted at the initial URL\n",
    "    for link in links:\n",
    "        href = link.get('href')\n",
    "        if href and not href.startswith('http') and not href.startswith('https'):\n",
    "            full_url = f'{urlparse(url).scheme}://{urlparse(url).netloc}{href}'\n",
    "            if full_url not in visited_urls:\n",
    "                visited_urls.add(full_url)\n",
    "                # Check if the link is on the same site as the initial URL\n",
    "                if urlparse(full_url).netloc == urlparse(url).netloc:\n",
    "                    # Add the full URL to the list of website URLs to visit\n",
    "                    website_urls.append(full_url)\n",
    "\n",
    "# Open a file to write the output\n",
    "with open('output.txt', 'w') as f:\n",
    "    if not pr_links:\n",
    "        f.write(\"No press releases found\\n\")\n",
    "    else:\n",
    "        f.write(f\"Found {len(pr_links)} press releases\\n\")\n",
    "        # Loop through each press release link and scrape the content\n",
    "        for pr_link in pr_links:\n",
    "            # Send a GET request to the press release link and store the HTML response\n",
    "            pr_response = requests.get(pr_link)\n",
    "\n",
    "            # Parse the HTML content using BeautifulSoup\n",
    "            pr_soup = BeautifulSoup(pr_response.content, 'html.parser')\n",
    "        # Extract the relevant information from the press release\n",
    "        title = pr_soup.find('h1').text\n",
    "        date = pr_soup.find('span', {'class': 'date'}).text\n",
    "        content = pr_soup.find('div', {'class': 'content'}).text\n",
    "\n",
    "        # Write the extracted information to the file\n",
    "        f.write(f'Title: {title}\\n')\n",
    "        f.write(f'Date: {date}\\n')\n",
    "        f.write(f'Content: {content}\\n')\n",
    "        f.write('------------------------\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
